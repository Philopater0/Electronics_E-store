# -*- coding: utf-8 -*-
"""API-Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gQc_OgRSOu_BrVVwHm6PbSKBUbFwWSFz
"""

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

import json

def clean_and_preprocess_data(file_path):
    # Clean and preprocess data
    df = pd.read_csv(file_path, encoding='latin1')

    # Selecting features and target for training
    features = df[['CPU_ranking', 'RAM (in GB)', 'gpu_benchmark', 'Price (in Indian Rupees)', 'company']]
    features = features.dropna()

    # Encoding categorical features
    encoder = OneHotEncoder(handle_unknown='ignore')
    features_encoded = encoder.fit_transform(features[['company']])

    # Scaling numerical features
    scaler = StandardScaler()
    numerical_features = features[['CPU_ranking', 'RAM (in GB)', 'gpu_benchmark', 'Price (in Indian Rupees)']]
    features_scaled = scaler.fit_transform(numerical_features)

    # Combining encoded and scaled features
    features_final = np.concatenate((features_scaled, features_encoded.toarray()), axis=1)
    print("<<<<<<<<<<<<<<<<<<<<<<< Star Model AI from team chatbotech >>>>>>>>>>>>>>>>>>>>>>>> ")

    return df, features_final, encoder, scaler


def apply_kmeans_clustering(df, features_final, n_clusters=3, random_state=42):
    # Apply K-means clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)
    df['cluster'] = kmeans.fit_predict(features_final)
    return df, kmeans

def train_logistic_regression(features_final, df_cluster, test_size=0.2, random_state=63):
    # Splitting the data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(features_final, df_cluster, test_size=test_size, random_state=random_state)

    # Training the Logistic Regression model
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)

    # Evaluating the model's performance on the testing set
    accuracy = model.score(X_test, y_test)
    print("Accuracy on testing set: {:.2f}%".format(accuracy * 100))

    return model


file_path = 'dataset.csv'
df, features_final, encoder, scaler = clean_and_preprocess_data(file_path)

df, kmeans = apply_kmeans_clustering(df, features_final)
model = train_logistic_regression(features_final, df['cluster'])
# Exchange rate of Indian Rupee to Egyptian Pound
exchange_rate_inr_to_egp = 0.37 # You can change this number based on the current exchange rate
# Convert the price of the laptop from the Indian rupee to the Egyptian pound
df['Price (in Egyptian Pounds)'] = df['Price (in Indian Rupees)'] * exchange_rate_inr_to_egp





def get_user_profile_cluster(user_profile_data, kmeans, scaler, encoder):
    # Encoding and scaling user profile data
    company_encoded = encoder.transform(user_profile_data[['company']])
    numerical_data = user_profile_data[['CPU_ranking', 'RAM (in GB)', 'gpu_benchmark', 'Price (in Indian Rupees)']]
    numerical_data_scaled = scaler.transform(numerical_data)

    # Combining encoded and scaled user profile data
    user_profile_final = np.concatenate((numerical_data_scaled, company_encoded.toarray()), axis=1)

    # Predict the cluster for user profile data
    user_profile_cluster = kmeans.predict(user_profile_final)
    return user_profile_cluster[0]

def content_based_recommendation(user_profile, cluster_data):
    # Calculate content-based similarity within the selected cluster
    similarity_list_content = (
        2 * (user_profile['Ram'] == cluster_data['RAM (in GB)']).astype(int) +
        (user_profile['Touch'] == cluster_data['Touchscreen']).astype(int) +
        1 / (1 + np.abs(user_profile['Inches_max'] - cluster_data['Screen Size (in inch)'])) +
        1 / (1 + np.linalg.norm(user_profile['cpu_max'] - cluster_data['CPU_ranking'])) +
        1 / (1 + np.linalg.norm(user_profile['GPU_max'] - cluster_data['gpu_benchmark'])) +
        5 / (1 + np.abs(user_profile['min_price'] - cluster_data['Price (in Indian Rupees)'])) +  # Update: Higher weight for price
        5 / (1 + np.abs(user_profile['max_price'] - cluster_data['Price (in Indian Rupees)'])) +  # Update: Higher weight for price
        (user_profile['OS'] == cluster_data['Operating System']).astype(int) +
       1 * (user_profile['company'] == cluster_data['company']).astype(int)
    )
    cluster_data['similarity_content'] = similarity_list_content
    top_matches_content = cluster_data.sort_values(by=['similarity_content', 'user rating'], ascending=[False, False]).head(5)
    return top_matches_content


def main(user_profile):
   
    user_profile_data = pd.DataFrame({
        'CPU_ranking': [user_profile['cpu_max']],
        'RAM (in GB)': [user_profile['Ram']],
        'gpu_benchmark': [user_profile['GPU_max']],
        'Price (in Indian Rupees)': [user_profile['min_price']],
        'company': [user_profile['company']]
    })
    # Convert the company variable to dummy variables
    user_profile_data = pd.get_dummies(user_profile_data, columns=['company'], drop_first=True)

    # Ensure the user profile data has the same feature names as the training data
    user_profile_data = user_profile_data.reindex(columns=df.columns, fill_value=0)

    user_profile_cluster = get_user_profile_cluster(user_profile_data, kmeans, scaler, encoder)
    cluster_data = df[df['cluster'] == user_profile_cluster]
    recommendations = content_based_recommendation(user_profile, cluster_data)
    print("------------- Recommended Laptops----------------")
    print(recommendations)
    # إعادة تعيين الفهرس لتضمين عمود الفهرس كجزء من DataFrame
    df_reset = recommendations.reset_index()


    selected_columns = ['Id', 'Price (in Egyptian Pounds)']

    
    df_selected = df_reset[selected_columns]

    
    json_selected_columns = df_selected.to_json(orient='values')

   
    json_ = recommendations.to_json(orient='values')
    #print(json_)
    print('Top 5 laptops matching user preferences with the highest ratings (Content-Based) are:')
    for i, row in recommendations.iterrows():
       
     print(f"{i + 1}- {row['name']} \nSimilarity: {row['similarity_content']:.4f}\nRating: {row['user rating']}\nPrice: {row['Price (in Egyptian Pounds)']:,.2f} EGP\n")

    return json_selected_columns


if __name__ == "__main__":
    main()



